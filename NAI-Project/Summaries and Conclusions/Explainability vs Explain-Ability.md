[[TODO]]
Complex models -> more understanding, less understandability

Brain not explainable (no stepping through of a composers formula based on its neural activity), satisfyingly smart models this likely unintrospectible, symbols emerge in the external interaction of complex neural systems, preimposing them may limit their ability or they become so complex interacting that they lose their clusterification of meaning.

But produces explanations: so we should research deep complex models that are not transparent at the neural level but communicate insight as a skillfull person would do. Brains outputting programs, in the hopes that they may not achieve similar complexity and start writing programs their own

((As shown for example in \[\[FIGARO]], engineered features, informed by simple music theory, still provide highly valuable contributions to latent music representations. I would be interested in automatically generating music theory: If music theoretical features are so valuable (and apparently they are) we should be able to construct models, that come up with them by them selves. Right now this is not the case, current machine learning techniques still fail at reproducing age old music theoretical insights. This is a strong pointer that there is still an inductive something missing which would be ableâ€¦))
